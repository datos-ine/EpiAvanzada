---
title: "**Estudios de cohortes**"
author: ""
date: ""
output:
  html_document:
    css: style.css
    toc: true
    toc_float: true
    toc_collapsed: false
    toc_depth: 4
number_sections: true
anchor_sections: true
theme: lumen
editor_options: 
  markdown: 
    wrap: 72
---

```{r, message=FALSE, echo=F}
knitr::opts_chunk$set(comment=NA, dpi = 300)
```

<br>

<center>

_Este material es parte de la_ **_Unidad 5 del Curso de Epidemiología -
Nivel Avanzado del Instituto Nacional de Epidemiología "Dr. Juan H.
Jara" - ANLIS_**

</center>

<br>

<center>

<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/">

<a property="dct:title" rel="cc:attributionURL" href="https://cballejo.github.io/R_Epi_Avanzada/Unidad5/cohortes/">Estudios de cohortes</a> by
<a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="http://www.ine.gov.ar">Andrea
Silva y Christian Ballejo</a> is licensed under
<a href="http://creativecommons.org/licenses/by-nc/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC
BY-NC
4.0<img src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"/><img src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"/><img src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"/></a>

</p>

</center>

<br>

## Introducción

En su concepción más simple, un diseño de cohorte implica la selección de un grupo de personas expuestas al factor de estudio y la selección de un grupo similar, pero sin la exposición, considerado como no expuesto. El seguimiento de ambos grupos se pautará para un lapso determinado, y se evaluará en dicho período si se produjo o no el evento de interés. Finalmente se realiza una comparación entre ambos grupos.

La esencia de los estudios de cohortes es el seguimiento. Por tratarse de estudios longitudinales, los mismos podrán ser prospectivos, retrospectivos o mixtos. La cohorte podrá ser una cohorte fija o dinámica. En las primeras, se fija un período de reclutamiento de participantes y una vez finalizado dicho período, no se incluyen más sujetos para el seguimiento. En una cohorte abierta, o dinámica, se permite el ingreso y egreso de participantes  en toda la extensión del estudio. 

Como recordarán, los criterios de selección están en la exposición, por lo cual la misma debe estar claramente definida. Como dijimos anteriormente, la idea clásica de un estudio de cohorte es comparar dos grupos, expuestos y no expuestos. Pero los grupos de referencia pueden ser otros: 

- **Estudios de comparación con la población general**: El estudio identifica y sigue a una determinada cohorte de exposición. La frecuencia de aparición de la enfermedad en dicha cohorte es comparada con la observada en la población general. Para ello es necesario disponer de registros poblacionales que suministren dicha información. 

- **Comparaciones internas**: La cohorte a estudio supone una mezcla de personas expuestas y no expuestas. Las comparaciones se realizan dentro de la propia cohorte.

Por otra parte, el evento a observar puede tomar distintas formas: puede tratarse de un evento fijo, eventos múltiples, cambio en una medida de función (la cual se evalúa mediante tasa de cambio) o marcadores intermedios del evento, esto también impactará en la forma de análisis.

A la hora de diseñar un estudio de cohortes es importante, entonces, definir de forma clara los siguientes aspectos:

- Criterios de inclusión
- Fecha de entrada y de salida de los participantes en el estudio
- Seguimiento
- Eventos finales de interés
- La exposición
- Factores de confusión
- Consideraciones éticas 
- Estrategias de análisis
- Potencia estadística


En la siguiente tabla se remarcan ventajas y desventajas de estos estudios.

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-amwm">Ventajas  </th>
    <th class="tg-amwm">Desventajas   </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax">• Más cercanos a un experimento<br>   <br>• La relación temporal causa efecto es verificable<br>   <br>• Se pueden estimar medidas de incidencia<br>   <br>• Eficientes para evaluar exposiciones poco frecuentes<br>   <br>• Se pueden estudiar varios eventos<br>   <br>• Se pueden fijar criterios de calidad en la medición del evento<br>   <br>• Bajo riesgo de sesgo de selección (en especial en estudios prospectivos)   </td>
    <td class="tg-0lax">• Cuando se trata de eventos poco frecuentes la complejidad y el costo pueden aumentar considerablemente, ya que requiere estudiar y seguir un número grande de participantes<br>   <br>• Son estudios difíciles de realizar<br>   <br>    </td>
  </tr>
</tbody>
</table>

### ¿Cómo analizar un estudio de cohortes?

Seguramente ustedes recordarán de otros cursos de epidemiología, que en su forma más simple, podemos calcular Riesgos Relativos (RR) en un estudio de cohortes. Vamos a profundizar otros aspectos de este diseño.

El análisis de los estudios de cohortes ha de tener en cuenta la variable tiempo. La existencia de pérdidas de participantes por diferentes motivos y la diferente duración del seguimiento de los distintos miembros de la cohorte, implican la necesidad de técnicas analíticas especiales que, a la vez, permitan considerar toda la información aportada por los integrantes de la cohorte. Existen dos estrategias diferentes de análisis en los estudios de cohortes: el análisis supervivencia y el análisis personas-tiempo, pero ambas requieren disponer de información precisa sobre el inicio y el final del seguimiento de cada participante y conocer el status del participante en el momento de finalización o salida del estudio (desarrollo o no el evento de interés). Asimismo será necesario definir una escala temporal adecuada.

La estrategia de **análisis de supervivencia**, la abordaremos en la próxima unidad, de la mano de los estudios experimentales, pero tengan presente que la podemos emplear en cualquier estudio de cohorte, que contemos con la información puntualizada anteriormente.

Las técnicas de **personas-año** utilizan como unidad de análisis estratos o grupos homogéneos en los que es razonable asumir una misma tasa de incidencia. Las técnicas clásicas de análisis “personas tiempo” incluyen la comparación de tasas, la estandarización directa y las razones estandarizadas de incidencia o mortalidad (denominadas en la literatura RIE o SIR, para la incidencia y RME o SMR para la mortalidad). RIE y RME comparan el número de casos observados con el número esperado teniendo en cuenta la frecuencia de enfermedad en la población utilizada como referencia. La técnica multivariada por excelencia para la modelización de las tasas es la regresión de Poisson, que utiliza la distribución de Poisson para modelizar la variabilidad aleatoria del numerador de las tasas y es lo que aprenderemos en este capítulo.

A modo de resumen, compartimos esta tabla, extraída del libro de Hernández Avila:

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cnoz{background-color:#C0C0C0;text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-nrix{text-align:center;vertical-align:middle}
.tg .tg-byb7{background-color:#C0C0C0;text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0lax">&nbsp;&nbsp;&nbsp;<br> &nbsp;&nbsp;&nbsp;</th>
    <th class="tg-nrix"><span style="color:black">Análisis de supervivencia</span>   </th>
    <th class="tg-nrix"><span style="color:black">Análisis basado en tiempo-persona</span> </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-cnoz"><span style="color:black">Tamaño de la muestra</span></td>
    <td class="tg-byb7"><span style="color:black">Relativamente pequeño</span><span style="background-color:silver">  </span></td>
    <td class="tg-byb7"><span style="color:black">Relativamente grande</span><span style="background-color:silver">   </span></td>
  </tr>
  <tr>
    <td class="tg-0lax"><span style="color:black">Tipo de eventos</span>   </td>
    <td class="tg-0lax"><span style="color:black">Frecuentes</span>   </td>
    <td class="tg-0lax"><span style="color:black">Raros</span>   </td>
  </tr>
  <tr>
    <td class="tg-byb7"><span style="color:black">Escala temporal</span><span style="background-color:silver">   </span></td>
    <td class="tg-byb7"><span style="color:black">Única</span><span style="background-color:silver">   </span></td>
    <td class="tg-byb7"><span style="color:black">Única o múltiple</span><span style="background-color:silver">   </span></td>
  </tr>
  <tr>
    <td class="tg-0lax"><span style="color:black">Tipos de medida</span>  </td>
    <td class="tg-0lax"><span style="color:black">Probabilidad  (condicional y acumulada)</span>   </td>
    <td class="tg-0lax"><span style="color:black">Tasa (densidad)</span>   </td>
  </tr>
  <tr>
    <td class="tg-byb7"><span style="color:black">Análisis bivariado</span><span style="background-color:silver"> </span><br></td>
    <td class="tg-byb7"><span style="color:black">Comparación de curvas de supervivencia</span><br><br><span style="background-color:#C0C0C0">Lo</span><span style="color:black">g-rank test</span><br><span style="background-color:silver">   </span><br><span style="color:black">Razón de riesgos</span><span style="background-color:silver">   </span></td>
    <td class="tg-byb7"><span style="color:black">Comparación de tasas</span><br><br><span style="color:black">Razón de tasas</span><br><span style="background-color:silver">   </span><br><span style="color:black">REM</span><span style="background-color:silver">   </span></td>
  </tr>
  <tr>
    <td class="tg-0lax"><span style="color:black">Análisis multivariado</span>   </td>
    <td class="tg-0lax"><span style="color:black">Regresión de Cox</span>   </td>
    <td class="tg-0lax"><span style="color:black">Regresión de Poisson</span>   </td>
  </tr>
</tbody>
</table>

## Regresión de Poisson

La distribución de Poisson es una distribución que modeliza bien situaciones de conteo. Por ejemplo: números de accidentes, número de personas que tienen un infarto, número de personas que asisten a una consulta, número de hijos, etc. La característica es que son números finitos, no muy grandes, siempre positivos. Los mismos transcurren dentro de un intervalo, habitualmente de tiempo, pero también se puede considerar intervalos de población. Es decir que busca modelar el número de veces en que ocurre un evento en un intervalo determinado. 

La distribución de Poisson toma, pues, valores enteros no negativos: 0, 1, 2, 3, 4... La distribución de Poisson tiene un único parámetro,  lambda ($\lambda$), que coincide con la Esperanza y la Varianza de la distribución. O sea, es una distribución que cuanto más grande es el valor esperado más dispersión tienen los valores.

Vamos a entender cómo se aplica la regresión de Poisson en el marco de un estudio de cohortes. Supongamos entonces que tenemos un estudio de cohortes clásico, con dos grupos de comparación: un grupo expuesto y otro no expuesto. Para cada grupo se dispone el número de eventos ($d$) y la cantidad de personas-tiempo seguidas ($n$). Como recordarán, la tasa de incidencia ($\lambda$), la podemos calcular como el cociente: 

$$\lambda = \frac{d}{n}$$
Se asume que el número de eventos observados en cada grupo sigue una distribución de Poisson, con un valor esperado igual al producto de la tasa de incidencia por las personas tiempo. $E(d)= \lambda*n$. Así, la probabilidad de observar $d$ eventos, sería:

$$P(x=d)= \frac{(\lambda n)^d e^{-\lambda n}}{d} $$

Omitiendo el desarrollo matemático, la modelización de la regresión de Poisson consiste en establecer un modelo de efectos lineales de distintas covariables sobre el logaritmo de la tasa del subgrupo correspondiente.En general, el modelo de Poisson se expresa como: 

$$ln\lambda = \alpha+\beta_1x_1+\beta_2x_2+\dots+\beta_nx_n \qquad (*)$$ 

En forma equivalente, podemos decir:

$$\lambda=e^{\beta_0+\beta_1x_1+\dots+\beta_nx_n} $$
Si operamos en forma análoga a lo que hicimos en la regresión logística, concluiremos que los parámetros del modelo son interpretables a través de potencias de base $e$ como riesgos relativos (cocientes de tasas). La expresión formal es:

$$RR_{x*/x}=e^{\sum^k_i=1\beta_i(x^*_i-x_i)}=\prod^k_{i=1}e^{\beta_i(x^*_i-x_i)} $$
El símbolo $\prod$ significa productoria. Se simboliza con la letra Pi en mayúscula e implica una secuencia de productos.

Nuevamente, si volvemos a la ecuación: 

$$\lambda = \frac{d}{n}$$

Entonces, si aplicamos logaritmo natural:

$$ln\lambda=ln \frac{d}{n}=ln(d)-ln(n)$$
Si igualamos con (*)

$$ln\lambda=\alpha+\beta_1x_1+\beta_2x_2+\dots+\beta_nx_n$$
$$ln(d)=ln(n)+\alpha+\beta_1x_1+\beta_2x_2+\dots+\beta_nx_n $$
Al término $ln(n)$ se lo llama **offset**. En general, es un valor que debemos darle al software para que ajuste un modelo de Poisson, no se estima a partir de los datos.

Ahora que tenemos la ecuación del modelo, será necesario estimar los coeficientes y luego evaluar la calidad del ajuste.

Los coeficientes se calculan utilizando métodos como la Estimación de máxima verosimilitud (MLE). A partir de los coeficientes que nos produce la estimación de máximo-verosimilitud podemos realizar las siguientes inferencias: el test de Wald, para testear la $H_0 : β_i=0$, sus IC, y de ahí calcular los RR, con sus respectivos IC.

La bondad de ajuste del modelo se evaluará a través de la función Deviance siguiendo el mismo esquema jerárquico de evaluación que en el caso de la regresión logística

### Supuestos del modelo de Poisson

Como dijimos, la idea de Poisson es contar eventos en intervalos de tiempo o de población (también podría ser espacial). La variable desenlace toma valores positivos. Depende de un solo parámetro: $\lambda$. La distribución de Poisson tiene iguales la media y la varianza. Este es un supuesto muy importante de este modelo, y se conoce como supuesto de equidispersión. Si la variación de los casos observados en una población excede a la variación esperada por Poisson, se está ante la presencia de un problema conocido como sobredispersión y, en tal caso, la distribución binomial negativa es más adecuada (que no abordaremos aquí).

Los principales supuestos de Poisson son:

- Las observaciones deben ser independientes.
-	El parámetro $\lambda$ debe ser constante a lo largo del tiempo: esto se garantiza si se cumple con el principio de equidispersión.
-	La cantidad de eventos en un intervalo es proporcional al tamaño del intervalo
-	Dos o más eventos no pueden ocurrir en un mismo momento puntual

En la práctica, no siempre se cumple el principio de equidispersión, y lo más frecuente es la sobredispersión, es decir que la varianza sea superior a la media. Esto conduce a sesgos en la estimación de los desvíos estándares de los coeficientes. Por eso es importante diagnosticar la sobredispersión. *Lindsay* propone aplicar el coeficiente de variación (CV) como indicador para evaluar sobredispersión. El CV se define como el cociente entre la varianza estimada y la media estimada. Otros autores sugieren otros indicadores, veremos en el ejemplo como podemos evaluar sobredispersión en R.

Otras situaciones en las cuales el modelo de Poisson no es adecuado es cuando existe un número excesivo de ceros, esto es, una frecuencia observada de ceros que no es consistente con el modelo Poisson. Esto se debe a que el $ln(0)$ no está definido.  Como se señaló previamente, si bien es cierto que el modelo binomial negativo se ha desarrollado para modelar la heterogeneidad no observada, también es cierto que esa misma heterogeneidad es originada por un número excesivo de ceros. De manera particular, es posible que el mecanismo aleatorio que dio origen a los datos de conteo muestre una mayor concentración para algún valor específico, que puede ser el cero (como ocurre con algunas variables vinculadas a salud) o cualquier otro valor positivo. Esto implica que dicho valor tiene una mayor probabilidad de ocurrencia que la especificada por la distribución Poisson o cualquier otra distribución. En estos casos, se suele utilizar un modelo para datos de conteo cero-inflado, que le confiere mayor peso a la probabilidad de que la variable de conteo sea igual a cero. 


## Aplicación en R

La regresión de Poisson se encuentra implementada en lenguaje R como otro de los modelos de la familia de modelos lineales generalizados (GLM de las siglas en inglés de *Generalized Linear Models*) similar a la regresión logística vista en la unidad anterior. 

Un criterio importante en la elección de la función de enlace para varias familias de distribuciones es asegurar que los valores ajustados del modelado permanezcan dentro de límites razonables. La especificación de un enlace logarítmico (predeterminado para Poisson) garantiza que los recuentos ajustados sean todos mayores o iguales a cero (propio de los conteos).

Por lo tanto, en este caso el ajuste tiene errores de Poisson y la función de enlace para linealizar la relación entre la variable respuesta y la(s) variable(s) independiente(s) es el `log`. 


### Construcción de un modelo de regresión de Poisson en R

Como es de imaginar utilizaremos la misma función general `glm()`, cambiando los argumentos en familia y enlace.

La sintaxis básica de esta función, contenida en el paquete `stats` de R base, es:

> glm(formula, family = poisson(link = "log"), data)

o lo mismo si omitimos el enlace, dado que el valor `log` es el predeterminado para esta familia.

> glm(formula, family = poisson), data)

donde:

__formula__: al igual que en la regresión logística o lineal, es la fórmula que describe el modelo a ajustar. Sigue la estructura:

$$variable\_dependiente \sim variable\_indepen_1 + variable\_indepen_2 +\dots+ variable\_indepen_n$$

__family__: hace referencia a la familia de distribuciones y, en link, a la función de enlace elegida para el ajuste de este modelo. 


__data__: indica el nombre de la base de datos (dataframe) que contiene las variables del modelo.


La salida de resultados de esta función puede obtenerse a través de la función `summary()` al igual que en las otras regresiones vistas. La sintaxis es `summary(nombre_modelo)`, siendo `nombre_modelo` el nombre del modelo ajustado. 

La resultados del objeto de regresión de Poisson mostrados están compuesto por:

__Call__: fórmula del modelo

__Deviance Residuals__: distribución de los residuos (mediana, mínimo, máximo y percentilos 25-75) obtenidos en la última iteración

__Coefficients__: coeficientes del Intercept y los asociados a cada variable independiente. Además se agregan los errores estándar y el valor z (*estadístico de Wald* que surge del cociente entre el coeficiente y su error estándar) con el p-valor correspondiente.

__Dispersion parameter for poisson family taken to be 1__: Significa que el modelo asume el supuesto de igualdad entre la media y la varianza (dispersión = 1)

__Null deviance__: devianza para el modelo nulo que solo contiene la constante.

__Residual deviance__: devianza del modelo ajustado.

__AIC__: criterio de información de Akaike.

__Number of Fisher Scoring iterations__: cantidad de iteraciones.

Este resumen surge del *objeto de regresión* construido cuando asignamos la salida de la función `glm()` que pertenece a la clase "glm" y "lm". 

El objeto de regresión está compuesto por 30 componentes que pueden accederse a través del nombre del modelo seguido del signo __$__ y el nombre del componente.  

Entre los componentes más relevantes podemos señalar (usamos *nombre_modelo* como generalización del nombre del objeto creado):

__nombre_modelo$coeficients__:  vector de coeficientes del modelo. También se puede obtener a través de la función `coef(nombre_modelo)`

__nombre_modelo$residuals__: vector que contiene los residuos obtenidos en la última iteración.

__nombre_modelo$fitted.values__: vector con los valores medios ajustados, obtenidos según
la transformación de los predictores lineales por la inversa de la función de enlace.

__nombre_modelo$family__: devuelve la familia utilizada en la construcción del modelo.

__nombre_modelo$deviance__: devianza del modelo ajustado o -2 veces el máximo de la log
verosimilitud.

__nombre_modelo$aic__: criterio de información de Akaike (AIC)

__nombre_modelo$null.deviance__: devianza para el modelo nulo que solo contiene la constante.

### Ejemplo práctico

A fin de mostrar las funciones y la metodología de análisis con R vamos a utilizar datos que se tomaron de un estudio de cohorte ocupacional realizado para probar la asociación entre las muertes respiratorias y la exposición al arsénico en la industria, después de ajustar por varios otros factores de riesgo. 

La variable principal es `muertes`. Este es el número de muertes por persona-años (persona.anio) de sujetos en cada categoría, por lo que finalmente nos interesa la tasa de incidencia (mortalidad) como variable resultado.

Las otras variables son covariables independientes que incluyen el grupo de edad `grupo.edad`, el período de empleo `periodo`, el año de inicio del empleo `comienzo` y el nivel de exposición al arsénico durante el período de estudio `arsenico`.

Activemos paquetes y leamos los datos.

```{r, message=F, warning=F}
library(tidyverse)

datos <- read_csv2("cohorte_ocupacional.csv")

glimpse(datos)
```
Observamos la estructura de los datos, donde vemos que los tipos de variables son todos numéricos. 

Nos acercan el diccionario de datos de estas variables y entonces sabemos que algunas de ellas estan codificadas numéricamente pero responden a categorías. Es el caso de las últimas cuatro variables que tienen un numero entero.

Necesitamos decirle a R que los interprete como variables categóricas (factores) y adjunte etiquetas a cada uno de los niveles. Esto se puede hacer de la misma forma que vimos en unidades anteriores como gestión de variables.

```{r}
datos <- datos %>% 
  mutate(grupo.edad = factor(grupo.edad, labels=c("40-49","50-59","60-69","70-79")),
         periodo = factor(periodo, labels=c("1938-1949", "1950-1959", "1960-1969", "1970-1977")),
         comienzo = factor(comienzo, labels=c("< 1925", "1925 y post")),
         arsenico = factor(arsenico, labels=c("<1 año", "1-4 años","5-14 años", "15+ años"))) 
```

Veamos como quedaron los niveles de estos factores:

```{r}
levels(datos$grupo.edad)
levels(datos$periodo)
levels(datos$comienzo)
levels(datos$arsenico)
```

Exploremos los datos de la base organizando la información de persona-años por edad y período

```{r, eval=F}
datos %>% count(periodo, grupo.edad, wt = persona.anio) %>% 
  pivot_wider(names_from = grupo.edad, values_from = n)
```

```{r, echo=F, message=F, warning=F}
library(flextable)

set_flextable_defaults(big.mark = "")

datos %>% count(periodo, grupo.edad, wt = persona.anio) %>% 
  pivot_wider(names_from = grupo.edad, values_from = n) %>% 
  flextable() %>% autofit()
```
Ahora hacemos lo mismo para el número de muertes y calculamos la incidencia por 10000 personas-año para cada celda.

```{r, eval=F}
personas_años <- datos %>% count(periodo, grupo.edad, 
                                 wt = persona.anio,
                                 name = "p.a")

datos %>% count(periodo, grupo.edad, wt = muertes) %>% 
  left_join(personas_años, by = c("periodo", "grupo.edad")) %>% 
  mutate(incidencia = round(n/p.a*10000,2)) %>% dplyr::select(-n, -p.a)
```


```{r, echo=F}
personas_años <- datos %>% count(periodo, grupo.edad, 
                                 wt = persona.anio,
                                 name = "p.a")

datos %>% count(periodo, grupo.edad, wt = muertes) %>% 
  left_join(personas_años, by = c("periodo", "grupo.edad")) %>% 
  mutate(incidencia = round(n/p.a*10000,2)) %>% dplyr::select(-n, -p.a) %>% 
  flextable() %>% autofit()
```
Podemos verlo mejor en un gráfico:

```{r, echo=F, out.width="70%", fig.align="center"}
datos %>% count(periodo, grupo.edad, wt = muertes) %>% 
  left_join(personas_años, by = c("periodo", "grupo.edad")) %>% 
  mutate(incidencia = round(n/p.a*10000,2)) %>% dplyr::select(-n, -p.a) %>% 
  ggplot(aes(x = periodo, y = incidencia, color = grupo.edad, group = grupo.edad)) +
  geom_point() +
  geom_line() +
  labs(title = "Incidencia de muertes (mortalidad) por edad y período") +
  scale_color_brewer(palette = "Set1", name = "Edad") +
  xlab(label = "Período") +
  ylab(label = "10.000 personas-año")
```
Este gráfico muestra, como es esperable, que el grupo de mayor edad generalmente se asocia con un mayor riesgo de morir, pero necesitamos ajustar los efectos de las demás covariables para examinar mejor esta relación.

Dado que en este ejemplo nos interesa modelar la tasa de incidencia vamos a necesitar incorporar dentro del modelo un termino de desplazamiento (en inglés *"offset"*).

Creamos el modelo saturado para trazar iteraciones backward.

```{r}
modelo <- glm(muertes ~ periodo + grupo.edad + arsenico, offset = log(persona.anio),
              family = poisson,
              data = datos)
```

El término `offset = log(persona.anio)` permite que la variable `persona.anio` sea el denominador de los recuentos de `muertes`. Se necesita una transformación logarítmica ya que, para un modelo lineal generalizado de Poisson, la función enlace es el log.

Veamos la salida resumen del modelo:

```{r}
summary(modelo)
```
Este  primer modelo con todas las covarables muestra como significativas a casi todos los niveles de las variables respecto a las referencias.

Un problema del ajuste de la regresión de Poisson, es el no cumplimiento del supuesto "media es igual a la varianza". Generalmente se presente en forma de dispersión excesiva y a veces insuficiente, donde la varianza es mayor o menor que el valor medio, respectivamente.

Podemos probar esa bondad de ajuste del `mod1`, en función de la significancia de la sobredispersión de los errores de un modelo de Poisson, aplicando la función `poisgof()` del paquete **epiDisplay**.

```{r, message=F, warning=F}
library(epiDisplay)

poisgof(modelo)
```
El componente `$chisq` se calcula a partir de la deviance del modelo. Cómo toda prueba de bondad de ajuste buscamos que el valor p sea mayor a 0,05.

Si solo se desea el valor p, la salida se puede acortar.

```{r}
poisgof(modelo)$p.value 
```
Por ahora el valor de p indica un buen ajuste.

Ahora probamos quitar de a una variable en la primer tanda iterativa.

```{r}
mod1 <- glm(muertes ~ periodo + grupo.edad, offset = log(persona.anio),
              family = poisson,
              data = datos)

mod2 <- glm(muertes ~ periodo + arsenico, offset = log(persona.anio),
              family = poisson,
              data = datos)
         
mod3 <- glm(muertes ~ grupo.edad + arsenico, offset = log(persona.anio),
              family = poisson,
              data = datos)      
```

Para comparar performance podemos usar `compare_perfomance()` del paquete **performance**.

```{r}
library(performance)

compare_performance(modelo, mod1, mod2, mod3, metrics = "common")
```
Buscamos el menor AIC, que en este caso es mod3 (modelo con variables grupo.edad y arsenico)

En la segunda fase de iteración partimos de este modelo y quitamos otra variable, lo que signica en nuestro ejemplo construir regresiones simples:

```{r}
mod3.1 <- glm(muertes ~ grupo.edad, offset = log(persona.anio),
              family = poisson,
              data = datos) 

mod3.2 <- glm(muertes ~ arsenico, offset = log(persona.anio),
              family = poisson,
              data = datos) 
```

Volvemos a comparar:

```{r}
compare_performance(mod3, mod3.1, mod3.2, metrics = "common")
```
El mejor modelo continúa siendo `mod3`, por lo que vamos a probar su bondad de ajuste:

```{r}
poisgof(mod3)$p.value
```
El p valor del test confirma un buen ajuste.

```{r}
summary(mod3)
```
Todos los niveles de las variables son significativos respecto a sus niveles de referencia.

```{r}
round(exp(coef(mod3)),2)  # función exp()
```

Traducidos a riesgo, mediante la exponenciación de los coeficientes, observamos que los diferentes categorías de años de exposición al arsénico no varían entre sí. Esto nos hace pensar que puede valer la pena agruparlo en solo dos niveles.

```{r}
datos <- datos %>% mutate(arsenico2 = if_else(arsenico == "<1 año", "<1 año", "1+ años"),
                          arsenico2 = factor(arsenico2)) 

mod4 <- glm(muertes ~ grupo.edad + arsenico2,
                offset=log(persona.anio), 
               family=poisson, 
               data = datos)

compare_performance(mod3, mod4, metrics = "common")
```
La perfomance del `mod4` mejora respecto del anterior.

```{r}
poisgof(mod4)$p.value
```
El ajuste continúa siendo bueno.

```{r}
summary(mod4)
```
Las variables incluídas son significativas.

En esta etapa, aceptaríamos el `mod4` como el mejor modelo, ya que tiene el AIC más pequeño entre todos los modelos que hemos probado. 

```{r}
round(exp(coef(mod4)),2)  
```
Concluimos que la exposición al arsénico durante al menos un año aumentaría el riesgo de la muerte por enfermedad respiratoria en 2,25 veces con significación estadística.

Expliquemos de donde deriva este cálculo. En los modelos de Poisson de este tipo, la estructura de la compensación (offset) definida como  **log(persona-tiempo)** convierte al resultado en **log(densidad de incidencia)**.

La tabla creada al comienzo del ejemplo proporciona la densidad de incidencia bruta por grupo de edad y período. Cada uno de los modelos de regresión de Poisson anteriores se puede utilizar para calcular la densidad de incidencia prevista cuando se dan las variables del modelo. Por ejemplo, para calcular la densidad de incidencia de una población de 100.000 personas de entre 40 y 49 años 
que estuvieron expuestas al arsénico durante menos de un año utilizando `mod4`, podemos ejecutar:

```{r}
newdata <- as.data.frame(list(grupo.edad = "40-49",
                              arsenico2 = "<1 año", persona.anio = 100000))

predict(mod4, newdata, type="response") 
```
Esta población tendría una densidad de incidencia estimada de 33,26 por 100.000 personas-año.

En un estudio de casos y controles, como los vistos en la Unidad 4, el OR se utiliza para comparar la prevalencia de exposición entre casos y controles. En un estudio de cohorte, este valor es igual 
a la relación entre las probabilidades de contraer una enfermedad entre el grupo expuesto 
y el no expuesto. La razón de los riesgos para los dos grupos se denomina entonces "razón de riesgo" o "riesgo relativo".

En un estudio de cohorte real, los sujetos no siempre tienen la misma duración de seguimiento. El riesgo relativo ignora la duración del seguimiento. Por tanto, no es una buena medida de comparación del riesgo entre los dos grupos. 

En este ejercicio, todos los sujetos agrupan sus tiempos de seguimiento y este número se denomina "tiempo-persona", que luego se utiliza como denominador del evento, lo que da como resultado una "densidad de incidencia". 

Comparar la densidad de incidencia entre dos grupos de sujetos por su estado de exposición es más justo que comparar los riesgos crudos. La relación entre las densidades de incidencia de dos grupos se denomina razón de densidades de incidencia (RDI), que es una forma mejorada de riesgo relativo.

En el `mod4`, para calcular la razón de densidad de incidencia entre los sujetos expuestos al arsénico durante uno o más años frente a los expuestos durante menos de un año, podemos dividir la incidencia entre los primeros por la del segundo grupo.

```{r}
newdata <- as.data.frame(list(grupo.edad = c("40-49", "40-49"),
                              arsenico2 = c("<1 año", "1+ años"), 
                              persona.anio = c(100000, 100000)))

di <- predict(mod4, newdata, type="response")

rdi.arsenico <- di[2] / di[1]

rdi.arsenico 
```
El código anterior comienza agregando una nueva fila al dataframe **newdata** que tiene lo mismo que la primera fila, excepto que la variable `arsenico2` es "1+ años". 

A continuación, se calculan las respuestas o densidades de incidencia de las dos condiciones. 

La RDI se obtiene luego de la división de las densidades de incidencia para arsenico2 = "<1 año" 
con arsenico2 = "1+ años". Una forma más corta de obtener esta RDI es obtener el coeficiente de la variable específica `arsenico`, que es el quinto coeficiente en el modelo.

```{r}
exp(coef(mod4)[5])
```
El paquete epiDisplay incluye una función útil para obtener una salida completa del modelo.

```{r}
idr.display(mod4)
```

También podemos utilizar las funciones de los paquetes **parameters** y **see** para graficar esta salida.

```{r, out.width="60%", fig.align="center", message=F, warning=F}
library(parameters)
library(see)

plot(model_parameters(mod4, exponentiate = T))
```






## Bibliografía

Epidemiología. Diseño y análisis de estudios. Mauricio Hernández Ávila, 2007 - Editorial Médica Panamericana S.A.

Andreu Nolasco. Estadística avanzada en ciencias de la salud - Universidad de Alicante, 2016

Pérez Hoyos S. Introducción a la regresión de Poisson. Quaderns de saut pública i administració de serveis de salut, 17. Valencia: Escola Valenciana d´Estudis per a la Salut, 2001.

Salinas-Rodríguez Aarón, Manrique-Espinoza Betty, Sosa-Rubí Sandra G. Análisis estadístico para datos de conteo: aplicaciones para el uso de los servicios de salud. Salud pública Méx  [revista en la Internet,. 2009  Oct [citado  2021  Sep  12] ;  51( 5 ): 397-406. Disponible en: <http://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S0036-36342009000500007&lng=es>


R Core Team. R: A language and environment for statistical
computing. R Foundation for Statistical Computing, Vienna, Austria, 2021. URL
<https://www.R-project.org/>.

Wickham et al. Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, 2019 <https://doi.org/10.21105/joss.01686>


Lüdecke D, Ben-Shachar M, Patil I, Makowski D. “Extracting, Computing and Exploring the Parameters of Statistical Models using R.”, 2020
_Journal of Open Source Software_, *5*(53), 2445. doi: 10.21105/joss.02445
<https://doi.org/10.21105/joss.02445>.

Lüdecke, Patil, Ben-Shachar, Wiernik, Waggoner & Makowski. Visualisation Toolbox for 'easystats' and Extra Geoms, Themes and Color Palettes for 'ggplot2', 2020. CRAN. Available from
<https://easystats.github.io/see/>

Virasakdi Chongsuvivatwong. epiDisplay: Epidemiological Data Display Package. R
package version 3.5.0.1., 2018 <https://CRAN.R-project.org/package=epiDisplay>



